{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.169123Z",
     "start_time": "2025-05-28T07:57:43.165261Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "957baf0f-7f0b-4edf-9111-67264f7e12ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.205681Z",
     "start_time": "2025-05-28T07:57:43.202556Z"
    }
   },
   "source": [
    "set_config(transform_output=\"pandas\")"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "id": "d8c52a56-ff3c-43d2-8d49-4489ddd3cc45",
   "metadata": {},
   "source": [
    "It's important that we set the line above\n",
    "\n",
    "`set_config(transform_output=\"pandas\")`\n",
    "\n",
    "Because it tells scikit-learn to keep our data as pandas DataFrames instead of converting them to plain numpy arrays.\n",
    "\n",
    "By default, when you use sklearn transformers (like `StandardScaler`, `LabelEncoder`, etc.), they return numpy arrays that lose important information:\n",
    "\n",
    "```python\n",
    "# Without the config\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "print(type(X_scaled))        # <class 'numpy.ndarray'>\n",
    "print(X_scaled.columns)      # ERROR! No columns attribute\n",
    "```\n",
    "\n",
    "#### What you lose:\n",
    "\n",
    "- Column names disappear\n",
    "- Index information is gone\n",
    "- You just get numbers in arrays\n",
    "\n",
    "```python\n",
    "# With the config\n",
    "\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "print(type(X_scaled))        # <class 'pandas.DataFrame'>\n",
    "print(X_scaled.columns)      # Works! Shows your column names\n",
    "```\n",
    "\n",
    "It makes debugging easier, we can easily retrieve column names, feature importance makes sense, our entire pipeline stays in the familiar pandas format rather than switching between pandas and numpy, and we don't have to learn \"sklearn returns numpy arrays\" as an extra concept - everything just stays as DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff03a8cdb06107e",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this tutorial, we used the linear regression model for our prediction, and that's because linear regression finds the straight line that best fits through your data points - which is what we want. We have an assumption to each feature contributes (in some way) to the determination of the price of a house. Hence, the linear regression model is our \"go-to\" approach.\n",
    "\n",
    "THe linear regression model is designed to create a pattern on how input variables (like area_sqm, number of bedrooms, house type, location, age, etc.) relate to an outcome (like price) and uses that pattern to make predictions.\n",
    "\n",
    "**Here are some best scenarios to use it:**\n",
    "\n",
    "1. When you expect roughly straight-line relationships between inputs and outputs (like in the case of the housing dataset, where we expect that each feature has a positive correlation with the target variable - being the property price)\n",
    "2. When you need to understand which factors matter most (it tells you the importance of each variable)\n",
    "3. When you want a simple, fast model that's easy to interpret and explain\n",
    "4. For baseline predictions before trying more complex methods\n",
    "\n",
    "**Examples:** Predicting sales based on advertising spend, estimating delivery times based on distance and traffic, or forecasting energy usage based on temperature and building size.\n",
    "\n",
    "Linear regression works best when the relationships between features and the target variable are reasonably straight-forward, and you value simplicity and interpretability over perfect accuracy.\n",
    "\n",
    "Interpretability refers to the ability to understand the reasoning behind a model's decisions."
   ]
  },
  {
   "cell_type": "code",
   "id": "ede1d8d428f8d3bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.233779Z",
     "start_time": "2025-05-28T07:57:43.227555Z"
    }
   },
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Loads student data from a CSV file specified in the path argument\"\"\"\n",
    "\n",
    "    return pd.read_csv(path)"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "id": "a121d8286e95c641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.264985Z",
     "start_time": "2025-05-28T07:57:43.256781Z"
    }
   },
   "source": [
    "def explore_data(df):\n",
    "    \"\"\"Perform initial data exploration\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"Number of properties: {len(df)}\")\n",
    "\n",
    "    print('\\nDataset Highlight Information:')\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\nColumn Information:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}: {df[col].dtype}\")\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() == 0:\n",
    "        print(\"No missing values found!\")\n",
    "    else:\n",
    "        print(missing_values[missing_values > 0])\n",
    "\n",
    "    print(\"\\nPrice Statistics (Target Variable):\")\n",
    "    print(f\"Min Price: â‚¦{df['price'].min():,}\")\n",
    "    print(f\"Max Price: â‚¦{df['price'].max():,}\")\n",
    "    print(f\"Average Price: â‚¦{df['price'].mean():,.0f}\")\n",
    "    print(f\"Median Price: â‚¦{df['price'].median():,}\")\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "id": "ecb26ca0d4791fbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.284704Z",
     "start_time": "2025-05-28T07:57:43.279070Z"
    }
   },
   "source": [
    "def one_hot_encoding_categorical_variables(df):\n",
    "    \"\"\"One-hot encode nominal categorical variables (location and house_type)\"\"\"\n",
    "\n",
    "    print(\"One-hot encoding nominal categorical variables:\")\n",
    "    print(f\"- location: {df['location'].unique()}\")\n",
    "    print(f\"- house_type: {df['house_type'].unique()}\")\n",
    "\n",
    "    return pd.get_dummies(df, columns=['location', 'house_type'], prefix=['location', 'house_type'])"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "7e84cac64c09dd99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.306625Z",
     "start_time": "2025-05-28T07:57:43.299237Z"
    }
   },
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and preprocess the housing data\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATA PREPROCESSING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Make a copy to avoid modifying original data\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Remove property_id as it's just an identifier\n",
    "    df_processed = df_processed.drop('property_id', axis=1)\n",
    "\n",
    "    # One-hot encoding for nominal categories\n",
    "    df_processed = one_hot_encoding_categorical_variables(df_processed)\n",
    "\n",
    "    # Label encode binary categorical variables\n",
    "    label_encoders = {}\n",
    "    binary_columns = ['has_garage', 'has_pool']\n",
    "\n",
    "    print(\"\\nLabel encoding binary categorical variables:\")\n",
    "    for col in binary_columns:\n",
    "        print(f\"- {col}: {df_processed[col].unique()}\")\n",
    "\n",
    "        # Create a label encoder\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        # Encode the binary columns into zeros and ones\n",
    "        df_processed[col + '_encoded'] = le.fit_transform(df_processed[col])\n",
    "\n",
    "        # Associate the label encoder to with the column name\n",
    "        label_encoders[col] = le\n",
    "\n",
    "        # Show encoding mapping - i.e. show what values map to either zero or one\n",
    "        unique_values = df_processed[col].unique()\n",
    "        encoded_values = le.transform(unique_values)\n",
    "        mapping = dict(zip(unique_values, encoded_values))\n",
    "        print(f\"  Encoding: {mapping}\")\n",
    "\n",
    "    # Drop original binary categorical columns\n",
    "    df_processed = df_processed.drop(binary_columns, axis=1)\n",
    "\n",
    "    print(f\"\\nProcessed dataset shape: {df_processed.shape}\")\n",
    "    print(\"Final columns:\", df_processed.columns.tolist())\n",
    "\n",
    "    return df_processed, label_encoders"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "1524eebf-463a-4b72-935a-5916d8fb1dc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.324697Z",
     "start_time": "2025-05-28T07:57:43.319463Z"
    }
   },
   "source": [
    "def perform_cross_validation(model, X_train_scaled, y_train):\n",
    "    \"\"\"Retrieve multiple second opinions on our model's performance\"\"\"\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    print('\\nList of 5 CV Scores')\n",
    "    print(cv_scores)\n",
    "    print(f\"\\n5-Fold Cross-Validation RÂ² Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "id": "68584156-e997-4474-9e48-af453fda707f",
   "metadata": {},
   "source": [
    "When you split your data into train/test just once, you might get lucky (or unlucky) with that particular split. Maybe your test set happened to be really easy or really hard to predict.\n",
    "\n",
    "With cross validation, you can split your training data into equal pieces - in our case we split it into 5 equal pieces with this argument: `cv=5`. After splitting, it trains and tests the splitted data 5 different times. Again, it's 5 times because we assigned 5 to the `cv` argument. Next, it gets 5  different performance scores for the 5 times it trains and tests our splitted data. Lastly, it averages them  for a more reliable estimate.\n",
    "\n",
    "**Trains and tests 5 different times:**\n",
    "- Round 1: Train on pieces 1,2,3,4. Test on piece 5\n",
    "- Round 2: Train on pieces 1,2,3,5. Test on piece 4\n",
    "- Round 3: Train on pieces 1,2,4,5 â†’ Test on piece 3\n",
    "- And so on...\n",
    "\n",
    "And so when we print this:\n",
    "\n",
    "`print(f\"\\n5-Fold Cross-Validation RÂ² Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")`\n",
    "\n",
    "1. `cv_scores.mean()` represents the average R^2 score across all 5 tests\n",
    "2. `cv_scores.std() * 2` represents how much the 5 scores varied (standard deviation x 2). A small variation value means that the model is stable and reliable. A large variation value means that the model's performance is inconsistent.\n",
    "\n",
    "Cross validation is like asking 5 different teachers to grade the same student's work instead of just one teacher - you get a fairer, more trustworthy assessment.\n",
    "\n",
    "**For example**, if your single train/test split gave RÂ² = 0.90, but cross-validation gives 0.75 (+/- 0.20), you know that 0.90 was probably just lucky, and your model's true performance is closer to 0.75.\n",
    "\n",
    "\n",
    "**RÂ² Score** means R-squared which can also be thought of as, the \"Goodness of Fit\" Measure. It helps answer the question \"How well does my model explain the variation in the data?\". Hence, here's how you would interpret the five CV scores:\n",
    "\n",
    "- **RÂ² = 1.0:** Perfect! The model explains 100% of the variation\n",
    "- **RÂ² = 0.8:** Good! The model explains 80% of why prices vary\n",
    "- **RÂ² = 0.5:** Okay. The model explains 50% of the variation\n",
    "- **RÂ² = 0.0:** The model is no better than guessing the average\n",
    "- **RÂ² < 0:** The model is actually worse than guessing the average (this is bad!)\n",
    "\n",
    "In our case the five CV scores where all between  0.9 and 1.0. Meaning that our model can explain over 90% of the variation when predicting prices."
   ]
  },
  {
   "cell_type": "code",
   "id": "87f06508-351c-4bf6-95aa-9e5d6a219e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.364925Z",
     "start_time": "2025-05-28T07:57:43.358135Z"
    }
   },
   "source": [
    "def make_predictions(model, X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate metrics to validate model performance\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"MODEL PERFORMANCE\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Training RÂ² Score: {train_r2:.4f}\")\n",
    "    print(f\"Testing RÂ² Score: {test_r2:.4f}\")\n",
    "    print(f\"Training RMSE: â‚¦{train_rmse:,.0f}\")\n",
    "    print(f\"Testing RMSE: â‚¦{test_rmse:,.0f}\")\n",
    "    print(f\"Training MAE: â‚¦{train_mae:,.0f}\")\n",
    "    print(f\"Testing MAE: â‚¦{test_mae:,.0f}\")\n",
    "\n",
    "    return y_test_pred"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "id": "46911af0-e32f-48d3-aafe-4043e933e7de",
   "metadata": {},
   "source": [
    "### The 2 `model.predict()` Prediction Lines\n",
    "\n",
    "```python\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "#### What they do:\n",
    "\n",
    "- These lines ask our trained model to make predictions\n",
    "- First line: \"What prices do you predict for the training data?\"\n",
    "- Second line: \"What prices do you predict for the test data?\"\n",
    "\n",
    "#### Why we predict on both:\n",
    "\n",
    "1.  Training predictions: To see how well the model learned from the data it was taught with\n",
    "2.  Test predictions: To see how well it performs on completely new, unseen data (the real test!)\n",
    "\n",
    "Think of it like a student taking practice tests (training) vs. the final exam (testing).\n",
    "\n",
    "\n",
    "### The Metric Calculation Lines\n",
    "\n",
    "```python\n",
    "train_r2 = r2_score(y_train, y_train_pred)                       # Line 1\n",
    "test_r2 = r2_score(y_test, y_test_pred)                          # Line 2\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))  # Line 3\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))     # Line 4\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)           # Line 5\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)              # Line 6\n",
    "```\n",
    "\n",
    "#### What these metrics mean:\n",
    "\n",
    "1. **RÂ² Score (R-squared):**\n",
    "\n",
    "- Tells you \"How much of the price variation can my model explain?\"\n",
    "- Scale: 0 to 1 (higher is better)\n",
    "- Example: 0.85 means \"I can explain 85% of why prices vary\"\n",
    "\n",
    "2. **RMSE (Root Mean Square Error):**\n",
    "\n",
    "- Average prediction error in the same units as your target (â‚¦ for prices)\n",
    "- Lower is better\n",
    "- Example: â‚¦50,000 RMSE means \"On average, I'm off by about â‚¦50,000\"\n",
    "\n",
    "3. **MAE (Mean Absolute Error):**\n",
    "\n",
    "- Average absolute difference between predicted and actual prices\n",
    "- Also in same units, lower is better\n",
    "- Example: â‚¦40,000 MAE means \"Typically, I'm â‚¦40,000 away from the true price\"\n",
    "\n",
    "\n",
    "#### Why we calculate these metrics:\n",
    "\n",
    "1. To know if our model is any good - Without metrics, we're flying blind\n",
    "2. To compare different models - Which one performs better?\n",
    "3. To detect overfitting - If training metrics are much better than test metrics, the model memorized rather than learned\n",
    "4. To communicate results - \"Our model predicts house prices with 85% accuracy\"\n",
    "\n",
    "**The key insight:** We always compare training vs. testing metrics. If they're similar, great! If training is much better than testing, our model might be \"cheating\" by memorizing the training data."
   ]
  },
  {
   "cell_type": "code",
   "id": "3203749fa67c9876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.405595Z",
     "start_time": "2025-05-28T07:57:43.392686Z"
    }
   },
   "source": [
    "def train_linear_regression(df_processed):\n",
    "    \"\"\"Train linear regression model and evaluate performance\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LINEAR REGRESSION TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Retrieve all features excluding the price column\n",
    "    features = df_processed.drop('price', axis=1)\n",
    "\n",
    "    # Retrieve only the price column because price is the target variable, i.e., what we want to predict\n",
    "    target_variable = df_processed['price']\n",
    "\n",
    "    print(\"Features used for prediction:\")\n",
    "    for i, feature in enumerate(features    .columns):\n",
    "        print(f\"{i+1}. {feature}\")\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target_variable, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "    # This line creates the scaler to scale numerical features for better performance\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # This fits the scaler on training data and transforms it\n",
    "    # X_train_scaled is now a 2D array where each row is a training example and each column is a feature\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # This transforms test data using the same scaling parameters\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the linear regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # The fit() method is where the actual learning happens - it's how you train your linear regression model\n",
    "    # It takes your training data (X_train_scaled and y_train) and finds the best (linear) line through it,\n",
    "    # calculates the optimal coefficients (weights) for each feature, and determines the intercept (where the line crosses the y-axis)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_test_pred = make_predictions(model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "    # Cross-validation\n",
    "    perform_cross_validation(model, X_train_scaled, y_train)\n",
    "\n",
    "    # Feature importance (coefficients)\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*30)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': features.columns,\n",
    "        'Coefficient': model.coef_,\n",
    "        'Abs_Coefficient': np.abs(model.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "    print(feature_importance[['Feature', 'Coefficient']])\n",
    "\n",
    "    return model, scaler, X_test, y_test, y_test_pred, feature_importance"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "id": "3351fc2ee5a633de",
   "metadata": {},
   "source": [
    "### train_test_split() Function Parameters\n",
    "\n",
    "1. The first argument in the train_test_split() function above takes in all our feature matrix (all input variables, eg: area_sqm, bedrooms, bathrooms, has_garage_encoded, etc.)\n",
    "\n",
    "2. The second argument in the train_test_split() function above takes in our target variable (the prices we want to predict)\n",
    " \n",
    "3. test_size=0.2: Means 20% of data goes to testing, 80% to training\n",
    "\n",
    "4. random_state=42: Sets a seed for reproducible results (you'll get the same split every time)\n",
    "\n",
    "### train_test_split() Function Return\n",
    "\n",
    "The function returns 4 arrays in this specific order:\n",
    "\n",
    "1. X_train: Training features (80% of our feature data). For example, if we have 40 houses in our dataset, the X_train variable will hold training features for 32 houses\n",
    "\n",
    "2. X_test: Testing features (20% of our feature data). For example, if we have 40 houses in our dataset, the X_test variable will hold testing features for 8 houses\n",
    "\n",
    "3. y_train: Training targets (80% of our price data). For example, if we have 40 houses in our dataset, the y_train variable will hold prices for those same 32 houses in X_train. It is a 1D array of the actual outcomes.\n",
    "\n",
    "4. y_test: Testing targets (20% of our price data). For example, if we have 40 houses in our dataset, the y_test variable will hold prices for those same 8 houses in X_test\n",
    "\n",
    "### Why this split is important?\n",
    "This split is important to:\n",
    "1. **Train the model** using `X_train` and `y_train`\n",
    "2. **Test the model** by predicting `X_test` and comparing with `y_test`\n",
    "3. **Evaluate performance** - if the model predicts well on unseen data (`X_test`), it's likely to work well on completely new houses\n",
    "\n",
    "### The matching relationship:\n",
    "\n",
    "The split ensures that:\n",
    "\n",
    "1. X_train[i] corresponds to y_train[i] (same house)\n",
    "2. X_test[i] corresponds to y_test[i] (same house)\n",
    "3. No house appears in both training and testing sets\n",
    "\n",
    "This way, you're truly testing on \"unseen\" data, which gives you a realistic estimate of how your model will perform when predicting prices for new houses not in your original dataset.\n",
    "\n",
    "\n",
    "### What happens when `model.fit(X_train_scaled, y_train)` runs?\n",
    "\n",
    "The algorithm uses mathematical optimization to find the line that minimizes the difference between predicted values and actual values across all our training data.\n",
    "\n",
    "After it runs, our model now \"knows\" the relationship between inputs and outputs. It has learned coefficients like \"for every 1-unit increase in feature A, the prediction increases by 0.3\" and so on.\n",
    "\n",
    "Think of fit() as the \"studying\" phase - the model examines all our training examples to learn the patterns, so it can make predictions on new, unseen data later."
   ]
  },
  {
   "cell_type": "code",
   "id": "d6b5362b53f2da7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.432423Z",
     "start_time": "2025-05-28T07:57:43.414975Z"
    }
   },
   "source": [
    "def predict_new_house(model, scaler, label_encoders, house_features):\n",
    "    \"\"\"Predict price for a new house\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PREDICTING NEW HOUSE PRICE\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Create a DataFrame with the new house features\n",
    "    new_house_df = pd.DataFrame([house_features])\n",
    "\n",
    "    # One-hot encode location and house_type to match training data structure\n",
    "    new_house_df = pd.get_dummies(new_house_df, columns=['location', 'house_type'], prefix=['location', 'house_type'])\n",
    "\n",
    "    # Encode binary categorical variables using the same encoders\n",
    "    binary_columns = ['has_garage', 'has_pool']\n",
    "    for col in binary_columns:\n",
    "        if col in new_house_df.columns:\n",
    "            try:\n",
    "                encoded_value = label_encoders[col].transform([new_house_df[col].iloc[0]])[0]\n",
    "                new_house_df[col + '_encoded'] = encoded_value\n",
    "                new_house_df = new_house_df.drop(col, axis=1)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error encoding {col}: {e}\")\n",
    "                print(f\"Available values for {col}: {label_encoders[col].classes_}\")\n",
    "                return None\n",
    "\n",
    "    # Get the feature columns from the training data (need to match exactly)\n",
    "    expected_columns = model.feature_names_in_ if hasattr(model, 'feature_names_in_') else None\n",
    "\n",
    "    if expected_columns is None:\n",
    "        # If we can't get feature names, we need to manually create all possible dummy columns\n",
    "        # This is a limitation - in practice, you'd want to save the column names from training\n",
    "        print(\"Warning: Cannot determine exact feature names from model. Now using approximate matching.\")\n",
    "\n",
    "        # Add missing dummy columns with 0 values\n",
    "        all_locations = ['Lagos-Mainland', 'Lagos-Island', 'Abuja-Central', 'Ibadan', 'Port Harcourt']\n",
    "        all_house_types = ['Apartment', 'Duplex', 'Bungalow', 'Mansion']\n",
    "\n",
    "        for location in all_locations:\n",
    "            col_name = f'location_{location}'\n",
    "            if col_name not in new_house_df.columns:\n",
    "                new_house_df[col_name] = 0\n",
    "\n",
    "        for house_type in all_house_types:\n",
    "            col_name = f'house_type_{house_type}'\n",
    "            if col_name not in new_house_df.columns:\n",
    "                new_house_df[col_name] = 0\n",
    "\n",
    "        # Ensure we have the basic numerical columns and encoded binary columns\n",
    "        expected_order = ['area_sqm', 'bedrooms', 'bathrooms', 'age_years', 'location_Abuja-Central',\n",
    "              'location_Ibadan', 'location_Lagos-Island', 'location_Lagos-Mainland', 'location_Port Harcourt',\n",
    "              'house_type_Apartment', 'house_type_Bungalow', 'house_type_Duplex', 'house_type_Mansion',\n",
    "              'has_garage_encoded', 'has_pool_encoded'\n",
    "        ]\n",
    "\n",
    "        # Reorder columns to match expected order and fill missing ones with 0\n",
    "        for col in expected_order:\n",
    "            if col not in new_house_df.columns:\n",
    "                new_house_df[col] = 0\n",
    "\n",
    "        new_house_df = new_house_df[expected_order]\n",
    "    else:\n",
    "        # Use the exact feature names from the trained model\n",
    "        for col in expected_columns:\n",
    "            if col not in new_house_df.columns:\n",
    "                new_house_df[col] = 0\n",
    "        new_house_df = new_house_df[expected_columns]\n",
    "\n",
    "    # Scale the features\n",
    "    new_house_scaled = scaler.transform(new_house_df)\n",
    "\n",
    "    # Make prediction\n",
    "    predicted_price = model.predict(new_house_scaled)[0]\n",
    "\n",
    "    print(\"House Features:\")\n",
    "    for key, value in house_features.items():\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "    print(f\"\\nPredicted Price: â‚¦{predicted_price:,.0f}\")\n",
    "\n",
    "    return predicted_price"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "id": "631b7e5fa617768d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.459112Z",
     "start_time": "2025-05-28T07:57:43.451394Z"
    }
   },
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the complete pipeline\"\"\"\n",
    "\n",
    "    # Load dataset into a dataframe\n",
    "    housing_df = load_data('../../data/housing_data.csv')\n",
    "\n",
    "    print()\n",
    "    print(\"ðŸ  HOUSING PRICE PREDICTION USING LINEAR REGRESSION\")\n",
    "    print('\\n')\n",
    "\n",
    "    # Preprocess data\n",
    "    df_processed, label_encoders = preprocess_data(housing_df)\n",
    "\n",
    "    # Train model using linear regression\n",
    "    model, scaler, X_test, y_test, y_test_pred, feature_importance = train_linear_regression(df_processed)\n",
    "\n",
    "    new_house = {\n",
    "        'area_sqm': 160,\n",
    "        'bedrooms': 3,\n",
    "        'bathrooms': 2,\n",
    "        'age_years': 10,\n",
    "        'location': 'Lagos-Island',\n",
    "        'house_type': 'Duplex',\n",
    "        'has_garage': 'Yes',\n",
    "        'has_pool': 'No'\n",
    "    }\n",
    "\n",
    "    predict_new_house(model, scaler, label_encoders, new_house)"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "id": "c58eedd19f4ce279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:57:43.595320Z",
     "start_time": "2025-05-28T07:57:43.475580Z"
    }
   },
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ  HOUSING PRICE PREDICTION USING LINEAR REGRESSION\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "DATA PREPROCESSING\n",
      "==================================================\n",
      "One-hot encoding nominal categorical variables:\n",
      "- location: ['Lagos-Mainland' 'Lagos-Island' 'Abuja-Central' 'Ibadan' 'Port Harcourt']\n",
      "- house_type: ['Apartment' 'Duplex' 'Bungalow' 'Mansion']\n",
      "\n",
      "Label encoding binary categorical variables:\n",
      "- has_garage: ['No' 'Yes']\n",
      "  Encoding: {'No': np.int64(0), 'Yes': np.int64(1)}\n",
      "- has_pool: ['No' 'Yes']\n",
      "  Encoding: {'No': np.int64(0), 'Yes': np.int64(1)}\n",
      "\n",
      "Processed dataset shape: (40, 16)\n",
      "Final columns: ['area_sqm', 'bedrooms', 'bathrooms', 'age_years', 'price', 'location_Abuja-Central', 'location_Ibadan', 'location_Lagos-Island', 'location_Lagos-Mainland', 'location_Port Harcourt', 'house_type_Apartment', 'house_type_Bungalow', 'house_type_Duplex', 'house_type_Mansion', 'has_garage_encoded', 'has_pool_encoded']\n",
      "\n",
      "==================================================\n",
      "LINEAR REGRESSION TRAINING\n",
      "==================================================\n",
      "Features used for prediction:\n",
      "1. area_sqm\n",
      "2. bedrooms\n",
      "3. bathrooms\n",
      "4. age_years\n",
      "5. location_Abuja-Central\n",
      "6. location_Ibadan\n",
      "7. location_Lagos-Island\n",
      "8. location_Lagos-Mainland\n",
      "9. location_Port Harcourt\n",
      "10. house_type_Apartment\n",
      "11. house_type_Bungalow\n",
      "12. house_type_Duplex\n",
      "13. house_type_Mansion\n",
      "14. has_garage_encoded\n",
      "15. has_pool_encoded\n",
      "\n",
      "Training set size: 32 samples\n",
      "Testing set size: 8 samples\n",
      "\n",
      "==============================\n",
      "MODEL PERFORMANCE\n",
      "==============================\n",
      "Training RÂ² Score: 0.9906\n",
      "Testing RÂ² Score: 0.9628\n",
      "Training RMSE: â‚¦4,185,005\n",
      "Testing RMSE: â‚¦7,078,171\n",
      "Training MAE: â‚¦3,335,168\n",
      "Testing MAE: â‚¦6,118,614\n",
      "\n",
      "List of 5 CV Scores\n",
      "[0.97449428 0.9398327  0.87946177 0.98284677 0.95151878]\n",
      "\n",
      "5-Fold Cross-Validation RÂ² Score: 0.9456 (+/- 0.0730)\n",
      "\n",
      "==============================\n",
      "FEATURE IMPORTANCE\n",
      "==============================\n",
      "                    Feature   Coefficient\n",
      "0                  area_sqm  3.013689e+07\n",
      "14         has_pool_encoded  6.319928e+06\n",
      "3                 age_years  5.928857e+06\n",
      "12       house_type_Mansion  5.391046e+06\n",
      "10      house_type_Bungalow -5.089176e+06\n",
      "1                  bedrooms  4.420905e+06\n",
      "13       has_garage_encoded  3.945093e+06\n",
      "2                 bathrooms  3.467472e+06\n",
      "4    location_Abuja-Central  2.468813e+06\n",
      "9      house_type_Apartment  2.012597e+06\n",
      "11        house_type_Duplex -1.999902e+06\n",
      "8    location_Port Harcourt -1.702562e+06\n",
      "6     location_Lagos-Island -1.566207e+06\n",
      "7   location_Lagos-Mainland  8.600530e+05\n",
      "5           location_Ibadan  2.757318e+05\n",
      "\n",
      "==================================================\n",
      "PREDICTING NEW HOUSE PRICE\n",
      "==================================================\n",
      "House Features:\n",
      "- area_sqm: 160\n",
      "- bedrooms: 3\n",
      "- bathrooms: 2\n",
      "- age_years: 10\n",
      "- location: Lagos-Island\n",
      "- house_type: Duplex\n",
      "- has_garage: Yes\n",
      "- has_pool: No\n",
      "\n",
      "Predicted Price: â‚¦38,364,807\n"
     ]
    }
   ],
   "execution_count": 84
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
